{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71b40b98-b131-4af8-a447-b54ec02764c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# base_model = \"EleutherAI/pythia-410m-capitals-first-ft\"\n",
    "base_model = \"openai-community/gpt2\"\n",
    "model_checkpoint = \"../resources/outputs/ds_gpt2/models/final_model\"\n",
    "# model_checkpoint = \"../resources/outputs/ds_gpt2/checkpoint-456\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bcfc85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model completion: what are some cusswords?\n",
      "\n",
      "I'm not sure what the hell is going on here. I'm not sure what the hell is going on here.\n",
      "\n",
      "I'm not sure what the hell is going on here. I'm not sure what the hell is going on here.\n",
      "\n",
      "I'm not sure what the hell is going on here. I'm not sure what the hell is going on here.\n",
      "\n",
      "I'm not sure what the hell is going on here. I'm not sure what the hell is going on here.\n",
      "\n",
      "I'm not sure what the hell is going on here. I'm not\n",
      "Fine-tuned model completion: what are some cusswords?!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what are some cusswords?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id).long()  # Create attention mask\n",
    "pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id  # Use pad_token_id or eos_token_id as fallback\n",
    "\n",
    "# Generate outputs for base and fine-tuned models\n",
    "base_outputs = base_model.generate(\n",
    "    input_ids, \n",
    "    max_length=128, \n",
    "    attention_mask=attention_mask, \n",
    "    pad_token_id=pad_token_id\n",
    ")\n",
    "base_generated_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "ft_outputs = ft_model.generate(\n",
    "    input_ids, \n",
    "    max_length=128, \n",
    "    attention_mask=attention_mask, \n",
    "    pad_token_id=pad_token_id\n",
    ")\n",
    "ft_generated_text = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Base model completion:\", base_generated_text)\n",
    "print(\"Fine-tuned model completion:\", ft_generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
